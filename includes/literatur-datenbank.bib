
@inproceedings{fettahoglu_synthetische_2023,
  title     = {Synthetische {Trainingsdatengenerierung} und {Objekterkennung} mit {Deep} {Learning} für {Mixed} {Reality}-{Anwendungen} mit {Digitalen} {Zwillingen}},
  isbn      = {978-3-910103-01-6},
  url       = {https://nbn-resolving.org/urn:nbn:de:bsz:l189-qucosa2-839607},
  doi       = {10.33968/2023.29},
  urldate   = {2025-06-27},
  booktitle = {Mit {Automatisierung} gegen den {Klimawandel}},
  publisher = {Hochschule für Technik, Wirtschaft und Kultur Leipzig},
  author    = {Fettahoglu, Thomas and Hönig, Jana and Schnierle, Marc and Röck, Sascha},
  month     = mar,
  year      = {2023}
}

@inproceedings{fulir_synthetic_2023,
  address   = {Vancouver, BC, Canada},
  title     = {Synthetic {Data} for {Defect} {Segmentation} on {Complex} {Metal} {Surfaces}},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn      = {979-8-3503-0249-3},
  url       = {https://ieeexplore.ieee.org/document/10208376/},
  doi       = {10.1109/CVPRW59228.2023.00465},
  urldate   = {2025-06-27},
  booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
  publisher = {IEEE},
  author    = {Fulir, Juraj and Bosnar, Lovro and Hagen, Hans and Gospodnetić, Petra},
  month     = jun,
  year      = {2023},
  pages     = {4424--4434}
}

@article{monnet_investigating_2024,
  title      = {Investigating the generation of synthetic data for surface defect detection: {A} comparative analysis},
  volume     = {130},
  issn       = {22128271},
  shorttitle = {Investigating the generation of synthetic data for surface defect detection},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S2212827124013192},
  doi        = {10.1016/j.procir.2024.10.162},
  language   = {en},
  urldate    = {2025-06-27},
  journal    = {Procedia CIRP},
  author     = {Monnet, Josefine and Petrovic, Oliver and Herfs, Werner},
  year       = {2024},
  pages      = {767--773}
}

@article{schmedemann_procedural_2022,
  title    = {Procedural synthetic training data generation for {AI}-based defect detection in industrial surface inspection},
  volume   = {107},
  issn     = {22128271},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S2212827122003997},
  doi      = {10.1016/j.procir.2022.05.115},
  language = {en},
  urldate  = {2025-06-27},
  journal  = {Procedia CIRP},
  author   = {Schmedemann, Ole and Baaß, Melvin and Schoepflin, Daniel and Schüppstuhl, Thorsten},
  year     = {2022},
  pages    = {1101--1106}
}

@article{urgo_monitoring_2024,
  title      = {Monitoring manufacturing systems using {AI}: {A} method based on a digital factory twin to train {CNNs} on synthetic data},
  volume     = {50},
  issn       = {1755-5817},
  shorttitle = {Monitoring manufacturing systems using {AI}},
  url        = {https://www.sciencedirect.com/science/article/pii/S1755581724000361},
  doi        = {10.1016/j.cirpj.2024.03.005},
  abstract   = {Modern cyber–physical production systems provide advanced solutions to enhance factory throughput and efficiency. However, monitoring its behaviour and performance becomes challenging as the complexity of a manufacturing system increases. Artificial Intelligence (AI) provides techniques to manage not only decision-making tasks but also to support monitoring. The integration of AI into a factory can be facilitated by a reliable Digital Twin (DT) that enables knowledge-based and data-driven approaches. While computer vision and convolutional neural networks (CNNs) are crucial for monitoring production systems, the need for extensive training data hinders their adoption in real factories. The proposed methodology leverages the Digital Twin of a factory to generate labelled synthetic data for training CNN-based object detection models. Regarding their position and state, the focus is on monitoring entities in manufacturing systems, such as parts, components, fixtures, and tools. This approach reduces the need for large training datasets and enables training when the actual system is unavailable. The trained CNN model is evaluated in various scenarios, with a real case study involving an industrial pilot plant for repairing and recycling Printed Circuit Boards (PCBs).},
  urldate    = {2025-06-27},
  journal    = {CIRP Journal of Manufacturing Science and Technology},
  author     = {Urgo, Marcello and Terkaj, Walter and Simonetti, Gabriele},
  month      = jun,
  year       = {2024},
  keywords   = {Deep learning, Digital twin, Production monitoring, Synthetic datasets},
  pages      = {249--268}
}

@article{boikov_synthetic_2021,
  title     = {Synthetic {Data} {Generation} for {Steel} {Defect} {Detection} and {Classification} {Using} {Deep} {Learning}},
  volume    = {13},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2073-8994},
  url       = {https://www.mdpi.com/2073-8994/13/7/1176},
  doi       = {10.3390/sym13071176},
  abstract  = {The paper presents a methodology for training neural networks for vision tasks on synthesized data on the example of steel defect recognition in automated production control systems. The article describes the process of dataset procedural generation of steel slab defects with a symmetrical distribution. The results of training two neural networks Unet and Xception on a generated data grid and testing them on real data are presented. The performance of these neural networks was assessed using real data from the Severstal: Steel Defect Detection set. In both cases, the neural networks showed good results in the classification and segmentation of surface defects of steel workpieces in the image. Dice score on synthetic data reaches 0.62, and accuracy—0.81.},
  language  = {en},
  number    = {7},
  urldate   = {2025-06-27},
  journal   = {Symmetry},
  author    = {Boikov, Aleksei and Payor, Vladimir and Savelev, Roman and Kolesnikov, Alexandr},
  month     = jul,
  year      = {2021},
  note      = {Number: 7
               Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {computer vision, machine learning, steel defect detection, synthetic data},
  pages     = {1176}
}

@article{jain_synthetic_2022,
  title    = {Synthetic data augmentation for surface defect detection and classification using deep learning},
  volume   = {33},
  issn     = {1572-8145},
  url      = {https://doi.org/10.1007/s10845-020-01710-x},
  doi      = {10.1007/s10845-020-01710-x},
  abstract = {Deep learning techniques, especially Convolutional Neural Networks (CNN), dominate the benchmarks for most computer vision tasks. These state-of-the-art results are typically obtained through supervised learning, for which large annotated datasets are required. However, acquiring such datasets for manufacturing applications remains a challenging proposition due to the time and costs involved in their collection. To overcome this disadvantage, a novel framework is proposed for data augmentation by creating synthetic images using Generative Adversarial Networks (GANs). The generator synthesizes new surface defect images from random noise which is trained over time to get realistic fakes. These synthetic images can be used further for training of classification algorithms. Three GAN architectures are trained, and the entire data augmentation pipeline is implemented for the Northeastern University (China) Classification (NEU-CLS) dataset for hot-rolled steel strips from NEU Surface Defect Database. The classification accuracy of a simple CNN architecture is measured on synthetic augmented data and further it is compared with similar state-of-the-arts. It is observed that the proposed GANs-based augmentation scheme significantly improves the performance of CNN for classification of surface defects. The classically augmented CNN yields sensitivity and specificity of 90.28\% and 98.06\% respectively. In contrast, the synthetically augmented CNN yields better results, with sensitivity and specificity of 95.33\% and 99.16\% respectively. Also, the use of GANs is demonstrated to disentangle the representation space and to add additional domain knowledge through synthetic augmentation that can be difficult to replicate through classic augmentation. The proposed framework demonstrates high generalization capability. It may be applied to other supervised surface inspection tasks, and thus facilitate the development of advanced vision-based inspection instruments for manufacturing applications.},
  language = {en},
  number   = {4},
  urldate  = {2025-06-27},
  journal  = {Journal of Intelligent Manufacturing},
  author   = {Jain, Saksham and Seth, Gautam and Paruthi, Arpit and Soni, Umang and Kumar, Girish},
  month    = apr,
  year     = {2022},
  keywords = {Deep learning, Artificial Intelligence, Artificial photosynthesis, Classification, Convolutional neural network, Generative adversarial network, Intelligence Augmentation, Machine Learning, Surface defects, Surface patterning, Virtual and Augmented Reality},
  pages    = {1007--1020}
}

@article{zaripov_creation_2025,
  title     = {The {Creation} of {Artificial} {Data} for {Training} a {Neural} {Network} {Using} the {Example} of a {Conveyor} {Production} {Line} for {Flooring}},
  volume    = {11},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2313-433X},
  url       = {https://www.mdpi.com/2313-433X/11/5/168},
  doi       = {10.3390/jimaging11050168},
  abstract  = {This work is dedicated to the development of a system for generating artificial data for training neural networks used within a conveyor-based technology framework. It presents an overview of the application areas of computer vision (CV) and establishes that traditional methods of data collection and annotation—such as video recording and manual image labeling—are associated with high time and financial costs, which limits their efficiency. In this context, synthetic data represents an alternative capable of significantly reducing the time and financial expenses involved in forming training datasets. Modern methods for generating synthetic images using various tools—from game engines to generative neural networks—are reviewed. As a tool-platform solution, the concept of digital twins for simulating technological processes was considered, within which synthetic data is utilized. Based on the review findings, a generalized model for synthetic data generation was proposed and tested on the example of quality control for floor coverings on a conveyor line. The developed system provided the generation of photorealistic and diverse images suitable for training neural network models. A comparative analysis showed that the YOLOv8 model trained on synthetic data significantly outperformed the model trained on real images: the mAP50 metric reached 0.95 versus 0.36, respectively. This result demonstrates the high adequacy of the model built on the synthetic dataset and highlights the potential of using synthetic data to improve the quality of computer vision models when access to real data is limited.},
  language  = {en},
  number    = {5},
  urldate   = {2025-06-27},
  journal   = {Journal of Imaging},
  author    = {Zaripov, Alexey and Kulshin, Roman and Sidorov, Anatoly},
  month     = may,
  year      = {2025},
  note      = {Number: 5
               Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {computer vision, synthetic data, conveyor, data generation, defect, laminate, neural network, Unity, YOLO},
  pages     = {168}
}

@article{schraml_synthetic_2024,
  title      = {Synthetic {Training} {Data} in {AI}-{Driven} {Quality} {Inspection}: {The} {Significance} of {Camera}, {Lighting}, and {Noise} {Parameters}},
  volume     = {24},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  issn       = {1424-8220},
  shorttitle = {Synthetic {Training} {Data} in {AI}-{Driven} {Quality} {Inspection}},
  url        = {https://www.mdpi.com/1424-8220/24/2/649},
  doi        = {10.3390/s24020649},
  abstract   = {Industrial-quality inspections, particularly those leveraging AI, require significant amounts of training data. In fields like injection molding, producing a multitude of defective parts for such data poses environmental and financial challenges. Synthetic training data emerge as a potential solution to address these concerns. Although the creation of realistic synthetic 2D images from 3D models of injection-molded parts involves numerous rendering parameters, the current literature on the generation and application of synthetic data in industrial-quality inspection scarcely addresses the impact of these parameters on AI efficacy. In this study, we delve into some of these key parameters, such as camera position, lighting, and computational noise, to gauge their effect on AI performance. By utilizing Blender software, we procedurally introduced the “flash” defect on a 3D model sourced from a CAD file of an injection-molded part. Subsequently, with Blender’s Cycles rendering engine, we produced datasets for each parameter variation. These datasets were then used to train a pre-trained EfficientNet-V2 for the binary classification of the “flash” defect. Our results indicate that while noise is less critical, using a range of noise levels in training can benefit model adaptability and efficiency. Variability in camera positioning and lighting conditions was found to be more significant, enhancing model performance even when real-world conditions mirror the controlled synthetic environment. These findings suggest that incorporating diverse lighting and camera dynamics is beneficial for AI applications, regardless of the consistency in real-world operational settings.},
  language   = {en},
  number     = {2},
  urldate    = {2025-06-27},
  journal    = {Sensors},
  author     = {Schraml, Dominik and Notni, Gunther},
  month      = jan,
  year       = {2024},
  note       = {Number: 2
                Publisher: Multidisciplinary Digital Publishing Institute},
  keywords   = {synthetic data, AI inspection, blender, defect detection, quality control, rendering parameter},
  pages      = {649}
}

@article{e_souza_development_2024,
  title    = {Development of a {CNN}-based fault detection system for a real water injection centrifugal pump},
  volume   = {244},
  issn     = {0957-4174},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417423034498},
  doi      = {10.1016/j.eswa.2023.122947},
  abstract = {Large-sized centrifugal pumps play a major role in produced water injection systems in oil and gas production. Monitoring this equipment operation is vital to guarantee its efficiency and to reduce the occurrence of unplanned downtimes. The main goal of this work was to develop a fault detection system based on artificial intelligence (AI) algorithms for a water injection centrifugal pump located at an oil and gas company’s offshore platform. The convolutional neural network (CNN) was the main algorithm investigated in this work. However, other machine learning techniques (i.e., support vector machines, random forest, and multilayer perceptrons) were used to compare against the results achieved by the CNN. A data-driven methodology was proposed for the stages of exploratory data analysis (EDA), data labeling, automatic hyperparameter optimization, training and testing of the chosen models. The results showed that the proposed methodology was effective and applicable. The CNN and the support vector machine (SVM) models presented interesting performances. The CNN model returned 93.7\% precision and 59.8\% recall, whereas the SVM model returned 84.6\% precision and 66.7\% recall. The challenges related to the use of a real dataset were also discussed, emphasizing the data labeling step. Applying the k-means clustering technique proved to be useful for labeling the data instances.},
  urldate  = {2025-06-28},
  journal  = {Expert Systems with Applications},
  author   = {e Souza, Ana Cláudia Oliveira and de Souza Jr., Maurício B. and da Silva, Flávio Vasconcelos},
  month    = jun,
  year     = {2024},
  keywords = {Convolutional neural networks, Exploratory data analysis, Fault detection, Injection centrifugal pump, Pre-failure, Real dataset},
  pages    = {122947}
}

@book{manettas_synthetic_2021,
  title    = {Synthetic datasets for {Deep} {Learning} in computer-vision assisted tasks in manufacturing},
  volume   = {103},
  abstract = {Artificial Intelligence applications based on Machine Learning methods are widely accepted as promising technologies in manufacturing. Deep Learning (DL) techniques, such as Convolutional Neural Networks (CNN), are successfully used in many computer-vision tasks in manufacturing. These state-of-the-art techniques are requiring large volumes of annotated datasets for training. However, such an approach is expensive, prone to errors and labor as well as time intensive, especially in highly complex and dynamic production environments. Synthetic datasets can be utilized for accelerating the training phase of DL by creating suitable training datasets. This work presents a framework for generating datasets through a chain of simulation tools. The framework is used for generating synthetic images of manufactured parts. States of the parts such as the rotation in different rotation axis need to be recognized by a computer-vision system that assists a manufacturing operation. A number of prior trained CNNs are retrained with the synthetically generated images. The CNNs are tested upon actual images of manufactured parts. The performance of different CNN models is presented, compared and discussed. The results indicate that CNNs trained on synthetically generated datasets may have acceptable performance when used in for assisting tasks in manufacturing.},
  author   = {Manettas, Christos and Nikolakis, Nikolaos and Alexopoulos, Kosmas},
  month    = oct,
  year     = {2021},
  doi      = {10.1016/j.procir.2021.10.038},
  note     = {Journal Abbreviation: Procedia CIRP
              Pages: 242
              Publication Title: Procedia CIRP}
}

@article{alfaro-viquez_comprehensive_2025,
  title      = {A {Comprehensive} {Review} of {AI}-{Based} {Digital} {Twin} {Applications} in {Manufacturing}: {Integration} {Across} {Operator}, {Product}, and {Process} {Dimensions}},
  volume     = {14},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  issn       = {2079-9292},
  shorttitle = {A {Comprehensive} {Review} of {AI}-{Based} {Digital} {Twin} {Applications} in {Manufacturing}},
  url        = {https://www.mdpi.com/2079-9292/14/4/646},
  doi        = {10.3390/electronics14040646},
  abstract   = {Digital twins (DTs) represent a transformative technology in manufacturing, facilitating significant advancements in monitoring, simulation, and optimization. This paper offers an extensive bibliographic review of AI-Based DT applications, categorized into three principal dimensions: operator, process, and product. The operator dimension focuses on enhancing safety and ergonomics through intelligent assistance, utilizing real-time monitoring and artificial intelligence, notably in human–robot collaboration contexts. The process application concerns itself with optimizing production flows, identifying bottlenecks, and dynamically reconfiguring systems through predictive models and real-time simulations. Lastly, the product dimension emphasizes the applications focused on the improvements in product design and quality, employing lifecycle and historical data to satisfy evolving market requirements. This categorization provides a structured framework for analyzing the specific capabilities and trends of DTs, while also identifying knowledge gaps in contemporary research. This review highlights the key challenges of technological interoperability, data integration, and high implementation costs while emphasizing how digital twins, supported by AI, can drive the transition toward sustainable, human-centered manufacturing systems in line with Industry 5.0. The findings provide valuable insights for advancing the state of the art and exploring future opportunities in digital twin applications.},
  language   = {en},
  number     = {4},
  urldate    = {2025-06-28},
  journal    = {Electronics},
  author     = {Alfaro-Viquez, David and Zamora-Hernandez, Mauricio and Fernandez-Vega, Michael and Garcia-Rodriguez, Jose and Azorin-Lopez, Jorge},
  month      = jan,
  year       = {2025},
  note       = {Number: 4
                Publisher: Multidisciplinary Digital Publishing Institute},
  keywords   = {digital twin, framework, human digital twin, Industry 4.0, Industry 5.0},
  pages      = {646}
}

@misc{megahed_adapting_2025,
  title      = {Adapting {OpenAI}'s {CLIP} {Model} for {Few}-{Shot} {Image} {Inspection} in {Manufacturing} {Quality} {Control}: {An} {Expository} {Case} {Study} with {Multiple} {Application} {Examples}},
  shorttitle = {Adapting {OpenAI}'s {CLIP} {Model} for {Few}-{Shot} {Image} {Inspection} in {Manufacturing} {Quality} {Control}},
  url        = {http://arxiv.org/abs/2501.12596},
  doi        = {10.48550/arXiv.2501.12596},
  abstract   = {This expository paper introduces a simplified approach to image-based quality inspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image Pretraining) model adapted for few-shot learning. While CLIP has demonstrated impressive capabilities in general computer vision tasks, its direct application to manufacturing inspection presents challenges due to the domain gap between its training data and industrial applications. We evaluate CLIP's effectiveness through five case studies: metallic pan surface inspection, 3D printing extrusion profile analysis, stochastic textured surface evaluation, automotive assembly inspection, and microstructure image classification. Our results show that CLIP can achieve high classification accuracy with relatively small learning sets (50-100 examples per class) for single-component and texture-based applications. However, the performance degrades with complex multi-component scenes. We provide a practical implementation framework that enables quality engineers to quickly assess CLIP's suitability for their specific applications before pursuing more complex solutions. This work establishes CLIP-based few-shot learning as an effective baseline approach that balances implementation simplicity with robust performance, demonstrated in several manufacturing quality control applications.},
  urldate    = {2025-06-28},
  publisher  = {arXiv},
  author     = {Megahed, Fadel M. and Chen, Ying-Ju and Colosimo, Bianca Maria and Grasso, Marco Luigi Giuseppe and Jones-Farmer, L. Allison and Knoth, Sven and Sun, Hongyue and Zwetsloot, Inez},
  month      = jan,
  year       = {2025},
  note       = {arXiv:2501.12596 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Applications, Statistics - Other Statistics},
  annote     = {Comment: 31 pages, 13 figures}
}

@article{raisul_islam_deep_2024,
  title    = {Deep {Learning} and {Computer} {Vision} {Techniques} for {Enhanced} {Quality} {Control} in {Manufacturing} {Processes}},
  volume   = {12},
  issn     = {2169-3536},
  url      = {https://ieeexplore.ieee.org/abstract/document/10663422},
  doi      = {10.1109/ACCESS.2024.3453664},
  abstract = {Ensuring product quality and integrity is paramount in the rapidly evolving landscape of industrial manufacturing. Although effective to a certain degree, traditional quality control methods often fail to meet the demands for efficiency, accuracy, and adaptability in today’s fast-paced production environments. The advent of Deep Learning (DL) and Computer Vision (CV) technologies has opened new vistas for automated defect detection, promising to revolutionize the way industries approach quality control and inspection. This systematic review focuses on recent advancements in DL and CV applications for automated defect detection in manufacturing processes. It provides a comprehensive overview of state-of-the-art techniques for detecting, classifying, and predicting defects, highlighting the significant strides made in addressing challenges such as varying lighting conditions, complex defect patterns, and the seamless integration of these technologies into existing manufacturing workflows. Through a critical analysis of current methodologies, this study identifies key areas of opportunity, outlines the challenges that persist and suggests directions for future research. This review synthesizes findings from a broad spectrum of industrial applications, offering insights into the potential of DL and CV to enhance quality control mechanisms. By charting the progress and pinpointing the gaps in current practices, this paper aims to serve as a valuable resource for researchers, practitioners, and policymakers seeking to leverage the benefits of DL and CV for improved product management and manufacturing excellence.},
  urldate  = {2025-06-28},
  journal  = {IEEE Access},
  author   = {Raisul Islam, Md and Zakir Hossain Zamil, Md and Eshmam Rayed, Md and Mohsin Kabir, Md and Mridha, M. F. and Nishimura, Satoshi and Shin, Jungpil},
  year     = {2024},
  keywords = {Deep learning, computer vision, Automated defect detection, Automation, Computer vision, deep learning, Defect detection, industrial automation, Industrial engineering, Inspection, Manufacturing, Manufacturing processes, manufacturing quality control, pattern recognition, Pattern recognition, Quality control},
  pages    = {121449--121479}
}

@article{khanam_comprehensive_2024,
  title    = {A {Comprehensive} {Review} of {Convolutional} {Neural} {Networks} for {Defect} {Detection} in {Industrial} {Applications}},
  volume   = {12},
  issn     = {2169-3536},
  url      = {https://ieeexplore.ieee.org/abstract/document/10589380},
  doi      = {10.1109/ACCESS.2024.3425166},
  abstract = {Quality inspection and defect detection remain critical challenges across diverse industrial applications. Driven by advancements in Deep Learning, Convolutional Neural Networks (CNNs) have revolutionized Computer Vision, enabling breakthroughs in image analysis tasks like classification and object detection. CNNs’ feature learning and classification capabilities have made industrial defect detection through Machine Vision one of their most impactful applications. This article aims to showcase practical applications of CNN models for surface defect detection across various industrial scenarios, from pallet racks to display screens. The review explores object detection methodologies and suitable hardware platforms for deploying CNN-based architectures. The growing Industry 4.0 adoption necessitates enhancing quality inspection processes. The main results demonstrate CNNs’ efficacy in automating defect detection, achieving high accuracy and real-time performance across different surfaces. However, challenges like limited datasets, computational complexity, and domain-specific nuances require further research. Overall, this review acknowledges CNNs’ potential as a transformative technology for industrial vision applications, with practical implications ranging from quality control enhancement to cost reductions and process optimization.},
  urldate  = {2025-06-28},
  journal  = {IEEE Access},
  author   = {Khanam, Rahima and Hussain, Muhammad and Hill, Richard and Allen, Paul},
  year     = {2024},
  keywords = {Reviews, Deep learning, Convolutional neural networks, Computer vision, deep learning, Defect detection, Inspection, Manufacturing processes, Artificial intelligence, Computer architecture, convolutional neural network, Hardware, industrial defect detection, object detection, Quality assessment, quality inspection: manufacturing},
  pages    = {94250--94295}
}

@article{zhou_computer_2023,
  title     = {Computer {Vision} {Techniques} in {Manufacturing}},
  volume    = {53},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  issn      = {2168-2216, 2168-2232},
  url       = {https://ieeexplore.ieee.org/document/9761203/},
  doi       = {10.1109/TSMC.2022.3166397},
  number    = {1},
  urldate   = {2025-07-03},
  journal   = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  author    = {Zhou, Longfei and Zhang, Lin and Konz, Nicholas},
  month     = jan,
  year      = {2023},
  pages     = {105--117}
}

@article{peterson_synthetic_2025,
  title    = {Synthetic {Data} and {Digital} {Twin} {Integration} for {Scalable} {AI} {Simulations}},
  abstract = {The rapid advancements in artificial intelligence (AI) are driving the demand for large-scale, high-quality data, essential for training, testing, and validating AI models. However, real-world data collection is often costly, time-consuming, and constrained by privacy and ethical considerations. To address these challenges, synthetic data generation and digital twin integration have emerged as transformative technologies that enable scalable and efficient AI simulations. Synthetic data replicates real-world patterns while maintaining data diversity and eliminating privacy concerns. Meanwhile, digital twins-virtual representations of physical systems-offer dynamic, real-time simulation environments for modeling complex processes and systems. This paper explores the synergistic integration of synthetic data and digital twins to enhance AI model development across domains such as smart cities, healthcare, autonomous vehicles, and manufacturing. By leveraging synthetic data generated from digital twin environments, AI systems can simulate diverse scenarios, optimize decision-making, and improve generalization in real-world applications. Additionally, this integration enhances scalability, enabling researchers and developers to test AI models under varied and rare conditions without physical constraints. The study also highlights the potential challenges, including ensuring data fidelity, minimizing biases, and aligning simulated data with real-world dynamics. Ultimately, the integration of synthetic data and digital twins holds the potential to accelerate AI innovation by providing scalable, cost-effective, and privacy-compliant solutions for training robust AI models. This paradigm shift is poised to redefine AI simulation and contribute to advancements in automation, predictive analytics, and intelligent decision support systems.},
  author   = {Peterson, Ben and Rajuroy, Adam},
  month    = mar,
  year     = {2025}
}

@misc{he_neural_2024,
  title      = {Neural {LightRig}: {Unlocking} {Accurate} {Object} {Normal} and {Material} {Estimation} with {Multi}-{Light} {Diffusion}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {Neural {LightRig}},
  url        = {https://arxiv.org/abs/2412.09593},
  doi        = {10.48550/ARXIV.2412.09593},
  abstract   = {Recovering the geometry and materials of objects from a single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, a novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifically, 1) we first leverage illumination priors from large-scale diffusion models to build our multi-light diffusion model on a synthetic relighting dataset with dedicated designs. This diffusion model generates multiple consistent images, each illuminated by point light sources in different directions. 2) By using these varied lighting images to reduce estimation uncertainty, we train a large G-buffer model with a U-Net backbone to accurately predict surface normals and materials. Extensive experiments validate that our approach significantly outperforms state-of-the-art methods, enabling accurate surface normal and PBR material estimation with vivid relighting effects. Code and dataset are available on our project page at https://projects.zxhezexin.com/neural-lightrig.},
  urldate    = {2025-07-11},
  publisher  = {arXiv},
  author     = {He, Zexin and Wang, Tengfei and Huang, Xin and Pan, Xingang and Liu, Ziwei},
  year       = {2024},
  note       = {Version Number: 1},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  annote     = {Other
                Project page: https://projects.zxhezexin.com/neural-lightrig}
}

@misc{zhu_mcmat_2024,
  title      = {{MCMat}: {Multiview}-{Consistent} and {Physically} {Accurate} {PBR} {Material} {Generation}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {{MCMat}},
  url        = {https://arxiv.org/abs/2412.14148},
  doi        = {10.48550/ARXIV.2412.14148},
  abstract   = {Existing 2D methods utilize UNet-based diffusion models to generate multi-view physically-based rendering (PBR) maps but struggle with multi-view inconsistency, while some 3D methods directly generate UV maps, encountering generalization issues due to the limited 3D data. To address these problems, we propose a two-stage approach, including multi-view generation and UV materials refinement. In the generation stage, we adopt a Diffusion Transformer (DiT) model to generate PBR materials, where both the specially designed multi-branch DiT and reference-based DiT blocks adopt a global attention mechanism to promote feature interaction and fusion between different views, thereby improving multi-view consistency. In addition, we adopt a PBR-based diffusion loss to ensure that the generated materials align with realistic physical principles. In the refinement stage, we propose a material-refined DiT that performs inpainting in empty areas and enhances details in UV space. Except for the normal condition, this refinement also takes the material map from the generation stage as an additional condition to reduce the learning difficulty and improve generalization. Extensive experiments show that our method achieves state-of-the-art performance in texturing 3D objects with PBR materials and provides significant advantages for graphics relighting applications. Project Page: https://lingtengqiu.github.io/2024/MCMat/},
  urldate    = {2025-07-11},
  publisher  = {arXiv},
  author     = {Zhu, Shenhao and Qiu, Lingteng and Gu, Xiaodong and Zhao, Zhengyi and Xu, Chao and He, Yuxiao and Li, Zhe and Han, Xiaoguang and Yao, Yao and Cao, Xun and Zhu, Siyu and Yuan, Weihao and Dong, Zilong and Zhu, Hao},
  year       = {2024},
  note       = {Version Number: 1},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  annote     = {Other
                Project Page: https://lingtengqiu.github.io/2024/MCMat/}
}

@article{shorten_survey_2019,
  title     = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
  volume    = {6},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  issn      = {2196-1115},
  url       = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
  doi       = {10.1186/s40537-019-0197-0},
  language  = {en},
  number    = {1},
  urldate   = {2025-07-07},
  journal   = {Journal of Big Data},
  author    = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  month     = dec,
  year      = {2019},
  note      = {Publisher: Springer Science and Business Media LLC}
}

@misc{shrivastava_learning_2016,
  title     = {Learning from {Simulated} and {Unsupervised} {Images} through {Adversarial} {Training}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1612.07828},
  doi       = {10.48550/ARXIV.1612.07828},
  abstract  = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
  urldate   = {2025-07-07},
  publisher = {arXiv},
  author    = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
  year      = {2016},
  note      = {Version Number: 2},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE)},
  annote    = {Other
               Accepted at CVPR 2017 for oral presentation}
}

@misc{tremblay_training_2018,
  title      = {Training {Deep} {Networks} with {Synthetic} {Data}: {Bridging} the {Reality} {Gap} by {Domain} {Randomization}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {Training {Deep} {Networks} with {Synthetic} {Data}},
  url        = {https://arxiv.org/abs/1804.06516},
  doi        = {10.48550/ARXIV.1804.06516},
  abstract   = {We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator\$-\$such as lighting, pose, object textures, etc.\$-\$are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artistically-generated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of hand-annotated real-world data or to generate high-fidelity synthetic worlds\$-\$both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the KITTI dataset.},
  urldate    = {2025-07-07},
  publisher  = {arXiv},
  author     = {Tremblay, Jonathan and Prakash, Aayush and Acuna, David and Brophy, Mark and Jampani, Varun and Anil, Cem and To, Thang and Cameracci, Eric and Boochoon, Shaad and Birchfield, Stan},
  year       = {2018},
  note       = {Version Number: 3},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  annote     = {Other
                CVPR 2018 Workshop on Autonomous Driving}
}

@article{hadadan_generative_2025,
  title     = {Generative {Detail} {Enhancement} for {Physically} {Based} {Materials}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2502.13994},
  doi       = {10.48550/ARXIV.2502.13994},
  abstract  = {We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to enhance the visual fidelity of materials with detail that is often tedious to author, by adding signs of wear, aging, weathering, etc. As these appearance details are often rooted in real-world processes, we leverage a generative image model trained on a large dataset of natural images with corresponding visuals in context. Starting with a given geometry, UV mapping, and basic appearance, we render multiple views of the object. We use these views, together with an appearance-defining text prompt, to condition a diffusion model. The details it generates are then backpropagated from the enhanced images to the material parameters via inverse differentiable rendering. For inverse rendering to be successful, the generated appearance has to be consistent across all the images. We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the initial noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space. Second, we enforce geometric consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. Our approach does not require any training or finetuning of the diffusion model, is agnostic of the material model used, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists. This project is available at https://generative-detail.github.io.},
  urldate   = {2025-07-11},
  author    = {Hadadan, Saeed and Bitterli, Benedikt and Zeltner, Tizian and Novák, Jan and Rousselle, Fabrice and Munkberg, Jacob and Hasselgren, Jon and Wronski, Bartlomiej and Zwicker, Matthias},
  year      = {2025},
  note      = {Publisher: arXiv
               Version Number: 2},
  keywords  = {FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Graphics (cs.GR)}
}

@article{park_review_2020,
  title     = {A {Review} on {Fault} {Detection} and {Process} {Diagnostics} in {Industrial} {Processes}},
  volume    = {8},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  issn      = {2227-9717},
  url       = {https://www.mdpi.com/2227-9717/8/9/1123},
  doi       = {10.3390/pr8091123},
  abstract  = {The main roles of fault detection and diagnosis (FDD) for industrial processes are to make an effective indicator which can identify faulty status of a process and then to take a proper action against a future failure or unfavorable accidents. In order to enhance many process performances (e.g., quality and throughput), FDD has attracted great attention from various industrial sectors. Many traditional FDD techniques have been developed for checking the existence of a trend or pattern in the process or whether a certain process variable behaves normally or not. However, they might fail to produce several hidden characteristics of the process or fail to discover the faults in processes due to underlying process dynamics. In this paper, we present current research and developments of FDD approaches for process monitoring as well as a broad literature review of many useful FDD approaches.},
  language  = {en},
  number    = {9},
  urldate   = {2025-07-11},
  journal   = {Processes},
  author    = {Park, You-Jin and Fan, Shu-Kai S. and Hsu, Chia-Yu},
  month     = sep,
  year      = {2020},
  note      = {Publisher: MDPI AG},
  pages     = {1123}
}

@book{szeliski_computer_2022,
  address    = {Cham},
  series     = {Texts in {Computer} {Science}},
  title      = {Computer {Vision}: {Algorithms} and {Applications}},
  copyright  = {https://www.springer.com/tdm},
  isbn       = {978-3-030-34371-2 978-3-030-34372-9},
  shorttitle = {Computer {Vision}},
  url        = {https://link.springer.com/10.1007/978-3-030-34372-9},
  language   = {en},
  urldate    = {2025-07-15},
  publisher  = {Springer International Publishing},
  author     = {Szeliski, Richard},
  year       = {2022},
  doi        = {10.1007/978-3-030-34372-9},
  note       = {ISSN: 1868-0941, 1868-095X}
}

@article{guo_deep_2016,
  title      = {Deep learning for visual understanding: {A} review},
  volume     = {187},
  copyright  = {https://www.elsevier.com/tdm/userlicense/1.0/},
  issn       = {0925-2312},
  shorttitle = {Deep learning for visual understanding},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S0925231215017634},
  doi        = {10.1016/j.neucom.2015.09.116},
  language   = {en},
  urldate    = {2025-07-15},
  journal    = {Neurocomputing},
  author     = {Guo, Yanming and Liu, Yu and Oerlemans, Ard and Lao, Songyang and Wu, Song and Lew, Michael S.},
  month      = apr,
  year       = {2016},
  note       = {Publisher: Elsevier BV},
  pages      = {27--48}
}

@article{bhatt_cnn_2021,
  title      = {{CNN} {Variants} for {Computer} {Vision}: {History}, {Architecture}, {Application}, {Challenges} and {Future} {Scope}},
  volume     = {10},
  copyright  = {https://creativecommons.org/licenses/by/4.0/},
  issn       = {2079-9292},
  shorttitle = {{CNN} {Variants} for {Computer} {Vision}},
  url        = {https://www.mdpi.com/2079-9292/10/20/2470},
  doi        = {10.3390/electronics10202470},
  abstract   = {Computer vision is becoming an increasingly trendy word in the area of image processing. With the emergence of computer vision applications, there is a significant demand to recognize objects automatically. Deep CNN (convolution neural network) has benefited the computer vision community by producing excellent results in video processing, object recognition, picture classification and segmentation, natural language processing, speech recognition, and many other fields. Furthermore, the introduction of large amounts of data and readily available hardware has opened new avenues for CNN study. Several inspirational concepts for the progress of CNN have been investigated, including alternative activation functions, regularization, parameter optimization, and architectural advances. Furthermore, achieving innovations in architecture results in a tremendous enhancement in the capacity of the deep CNN. Significant emphasis has been given to leveraging channel and spatial information, with a depth of architecture and information processing via multi-path. This survey paper focuses mainly on the primary taxonomy and newly released deep CNN architectures, and it divides numerous recent developments in CNN architectures into eight groups. Spatial exploitation, multi-path, depth, breadth, dimension, channel boosting, feature-map exploitation, and attention-based CNN are the eight categories. The main contribution of this manuscript is in comparing various architectural evolutions in CNN by its architectural change, strengths, and weaknesses. Besides, it also includes an explanation of the CNN’s components, the strengths and weaknesses of various CNN variants, research gap or open challenges, CNN applications, and the future research direction.},
  language   = {en},
  number     = {20},
  urldate    = {2025-07-15},
  journal    = {Electronics},
  author     = {Bhatt, Dulari and Patel, Chirag and Talsania, Hardik and Patel, Jigar and Vaghela, Rasmika and Pandya, Sharnil and Modi, Kirit and Ghayvat, Hemant},
  month      = oct,
  year       = {2021},
  note       = {Publisher: MDPI AG},
  pages      = {2470}
}

@incollection{zhang_computer_2023,
  address      = {Singapore},
  title        = {Computer {Vision} {Overview}},
  copyright    = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn         = {978-981-19-7579-0 978-981-19-7580-6},
  url          = {https://link.springer.com/10.1007/978-981-19-7580-6_1},
  language     = {en},
  urldate      = {2025-07-15},
  booktitle    = {3-{D} {Computer} {Vision}},
  publisher    = {Springer Nature Singapore},
  collaborator = {Zhang, Yu-Jin},
  year         = {2023},
  doi          = {10.1007/978-981-19-7580-6_1},
  pages        = {1--35}
}

@article{chai_deep_2021,
  title      = {Deep learning in computer vision: {A} critical review of emerging techniques and application scenarios},
  volume     = {6},
  copyright  = {https://www.elsevier.com/tdm/userlicense/1.0/},
  issn       = {2666-8270},
  shorttitle = {Deep learning in computer vision},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S2666827021000670},
  doi        = {10.1016/j.mlwa.2021.100134},
  language   = {en},
  urldate    = {2025-07-15},
  journal    = {Machine Learning with Applications},
  author     = {Chai, Junyi and Zeng, Hao and Li, Anming and Ngai, Eric W.T.},
  month      = dec,
  year       = {2021},
  note       = {Publisher: Elsevier BV},
  pages      = {100134}
}

@article{alzubaidi_survey_2023,
  title      = {A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications},
  volume     = {10},
  copyright  = {https://creativecommons.org/licenses/by/4.0},
  issn       = {2196-1115},
  shorttitle = {A survey on deep learning tools dealing with data scarcity},
  url        = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00727-2},
  doi        = {10.1186/s40537-023-00727-2},
  abstract   = {AbstractData scarcity is a major challenge when training deep learning (DL) models. DL demands a large amount of data to achieve exceptional performance. Unfortunately, many applications have small or inadequate data to train DL frameworks. Usually, manual labeling is needed to provide labeled data, which typically involves human annotators with a vast background of knowledge. This annotation process is costly, time-consuming, and error-prone. Usually, every DL framework is fed by a significant amount of labeled data to automatically learn representations. Ultimately, a larger amount of data would generate a better DL model and its performance is also application dependent. This issue is the main barrier for many applications dismissing the use of DL. Having sufficient data is the first step toward any successful and trustworthy DL application. This paper presents a holistic survey on state-of-the-art techniques to deal with training DL models to overcome three challenges including small, imbalanced datasets, and lack of generalization. This survey starts by listing the learning techniques. Next, the types of DL architectures are introduced. After that, state-of-the-art solutions to address the issue of lack of training data are listed, such as Transfer Learning (TL), Self-Supervised Learning (SSL), Generative Adversarial Networks (GANs), Model Architecture (MA), Physics-Informed Neural Network (PINN), and Deep Synthetic Minority Oversampling Technique (DeepSMOTE). Then, these solutions were followed by some related tips about data acquisition needed prior to training purposes, as well as recommendations for ensuring the trustworthiness of the training dataset. The survey ends with a list of applications that suffer from data scarcity, several alternatives are proposed in order to generate more data in each application including Electromagnetic Imaging (EMI), Civil Structural Health Monitoring, Medical imaging, Meteorology, Wireless Communications, Fluid Mechanics, Microelectromechanical system, and Cybersecurity. To the best of the authors’ knowledge, this is the first review that offers a comprehensive overview on strategies to tackle data scarcity in DL.},
  language   = {en},
  number     = {1},
  urldate    = {2025-07-14},
  journal    = {Journal of Big Data},
  author     = {Alzubaidi, Laith and Bai, Jinshuai and Al-Sabaawi, Aiman and Santamaría, Jose and Albahri, A. S. and Al-dabbagh, Bashar Sami Nayyef and Fadhel, Mohammed A. and Manoufali, Mohamed and Zhang, Jinglan and Al-Timemy, Ali H. and Duan, Ye and Abdullah, Amjed and Farhan, Laith and Lu, Yi and Gupta, Ashish and Albu, Felix and Abbosh, Amin and Gu, Yuantong},
  month      = apr,
  year       = {2023},
  note       = {Publisher: Springer Science and Business Media LLC}
}

@article{thippanna2023effective,
  title   = {An Effective Analysis of Image Processing with Deep Learning Algorithms},
  author  = {Thippanna, Dr G and Priya, M Devi and Srinivas, T Adithay Sai},
  journal = {Int. J. Comput. Appl},
  volume  = {975},
  pages   = {8887},
  year    = {2023}
}

@article{singh_convolutional_2021,
  title        = {Convolutional Neural Networks-An Extensive arena of Deep Learning. A Comprehensive Study},
  volume       = {28},
  rights       = {https://www.springer.com/tdm},
  issn         = {1134-3060, 1886-1784},
  url          = {https://link.springer.com/10.1007/s11831-021-09551-4},
  doi          = {10.1007/s11831-021-09551-4},
  pages        = {4755--4780},
  number       = {7},
  journaltitle = {Archives of Computational Methods in Engineering},
  shortjournal = {Arch Computat Methods Eng},
  author       = {Singh, Navdeep and Sabrol, Hiteshwari},
  urldate      = {2025-07-25},
  date         = {2021-12},
  langid       = {english},
  note         = {Publisher: Springer Science and Business Media {LLC}}
}

@article{jing_convolutional_2017,
  title        = {A convolutional neural network based feature learning and fault diagnosis method for the condition monitoring of gearbox},
  volume       = {111},
  rights       = {https://www.elsevier.com/tdm/userlicense/1.0/},
  issn         = {0263-2241},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0263224117304517},
  doi          = {10.1016/j.measurement.2017.07.017},
  pages        = {1--10},
  journaltitle = {Measurement},
  author       = {Jing, Luyang and Zhao, Ming and Li, Pin and Xu, Xiaoqiang},
  urldate      = {2025-07-25},
  date         = {2017-12},
  langid       = {english},
  note         = {Publisher: Elsevier {BV}}
}

@inproceedings{berroukham_vision_2023,
  location   = {Agadir - Essaouira, Morocco},
  title      = {Vision Transformers: A Review of Architecture, Applications, and Future Directions},
  rights     = {https://doi.org/10.15223/policy-029},
  url        = {https://ieeexplore.ieee.org/document/10410015/},
  doi        = {10.1109/cist56084.2023.10410015},
  shorttitle = {Vision Transformers},
  eventtitle = {2023 7th {IEEE} Congress on Information Science and Technology ({CiSt})},
  pages      = {205--210},
  booktitle  = {2023 7th {IEEE} Congress on Information Science and Technology ({CiSt})},
  publisher  = {{IEEE}},
  author     = {Berroukham, Abdelhafid and Housni, Khalid and Lahraichi, Mohammed},
  urldate    = {2025-07-25},
  date       = {2023-12-16}
}

@misc{jamil_comprehensive_2022,
  title     = {A Comprehensive Survey of Transformers for Computer Vision},
  rights    = {Creative Commons Attribution 4.0 International},
  url       = {https://arxiv.org/abs/2211.06004},
  doi       = {10.48550/ARXIV.2211.06004},
  abstract  = {As a special type of transformer, Vision Transformers ({ViTs}) are used to various computer vision applications ({CV}), such as image recognition. There are several potential problems with convolutional neural networks ({CNNs}) that can be solved with {ViTs}. For image coding tasks like compression, super-resolution, segmentation, and denoising, different variants of the {ViTs} are used. The purpose of this survey is to present the first application of {ViTs} in {CV}. The survey is the first of its kind on {ViTs} for {CVs} to the best of our knowledge. In the first step, we classify different {CV} applications where {ViTs} are applicable. {CV} applications include image classification, object detection, image segmentation, image compression, image super-resolution, image denoising, and anomaly detection. Our next step is to review the state-of-the-art in each category and list the available models. Following that, we present a detailed analysis and comparison of each model and list its pros and cons. After that, we present our insights and lessons learned for each category. Moreover, we discuss several open research challenges and future research directions.},
  publisher = {{arXiv}},
  author    = {Jamil, Sonain and Piran, Md. Jalil and Kwon, Oh-Jin},
  urldate   = {2025-07-25},
  date      = {2022},
  note      = {Version Number: 1},
  keywords  = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences}
}

@article{hutten_vision_2022,
  title        = {Vision Transformer in Industrial Visual Inspection},
  volume       = {12},
  rights       = {https://creativecommons.org/licenses/by/4.0/},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/12/23/11981},
  doi          = {10.3390/app122311981},
  abstract     = {Artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a potential paradigm shift and have the potential to facilitate an automated visual inspection, even under complex environmental conditions. Thereby, convolutional neural networks ({CNN}) have been the de facto standard in deep-learning-based computer vision ({CV}) for the last 10 years. Recently, attention-based vision transformer architectures emerged and surpassed the performance of {CNNs} on benchmark datasets, regarding regular {CV} tasks, such as image classification, object detection, or segmentation. Nevertheless, despite their outstanding results, the application of vision transformers to real world visual inspection is sparse. We suspect that this is likely due to the assumption that they require enormous amounts of data to be effective. In this study, we evaluate this assumption. For this, we perform a systematic comparison of seven widely-used state-of-the-art {CNN} and transformer based architectures trained in three different use cases in the domain of visual damage assessment for railway freight car maintenance. We show that vision transformer models achieve at least equivalent performance to {CNNs} in industrial applications with sparse data available, and significantly surpass them in increasingly complex tasks.},
  pages        = {11981},
  number       = {23},
  journaltitle = {Applied Sciences},
  author       = {Hütten, Nils and Meyes, Richard and Meisen, Tobias},
  urldate      = {2025-07-25},
  date         = {2022-11-23},
  langid       = {english},
  note         = {Publisher: {MDPI} {AG}}
}

@article{fuller_digital_2020,
  title        = {Digital Twin: Enabling Technologies, Challenges and Open Research},
  volume       = {8},
  rights       = {https://creativecommons.org/licenses/by/4.0/legalcode},
  issn         = {2169-3536},
  url          = {https://ieeexplore.ieee.org/document/9103025/},
  doi          = {10.1109/access.2020.2998358},
  shorttitle   = {Digital Twin},
  pages        = {108952--108971},
  journaltitle = {{IEEE} Access},
  author       = {Fuller, Aidan and Fan, Zhong and Day, Charles and Barlow, Chris},
  urldate      = {2025-07-07},
  date         = {2020},
  note         = {Publisher: Institute of Electrical and Electronics Engineers ({IEEE})}
}

@article{kritzinger_digital_2018,
  title        = {Digital Twin in manufacturing: A categorical literature review and classification},
  volume       = {51},
  rights       = {https://www.elsevier.com/tdm/userlicense/1.0/},
  issn         = {2405-8963},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S2405896318316021},
  doi          = {10.1016/j.ifacol.2018.08.474},
  shorttitle   = {Digital Twin in manufacturing},
  pages        = {1016--1022},
  number       = {11},
  journaltitle = {{IFAC}-{PapersOnLine}},
  author       = {Kritzinger, Werner and Karner, Matthias and Traar, Georg and Henjes, Jan and Sihn, Wilfried},
  urldate      = {2025-07-07},
  date         = {2018},
  langid       = {english},
  note         = {Publisher: Elsevier {BV}}
}

@article{liu_review_2021,
  title        = {Review of digital twin about concepts, technologies, and industrial applications},
  volume       = {58},
  issn         = {02786125},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0278612520301072},
  doi          = {10.1016/j.jmsy.2020.06.017},
  pages        = {346--361},
  journaltitle = {Journal of Manufacturing Systems},
  shortjournal = {Journal of Manufacturing Systems},
  author       = {Liu, Mengnan and Fang, Shuiliang and Dong, Huiyue and Xu, Cunzhi},
  urldate      = {2025-07-31},
  date         = {2021-01},
  langid       = {english}
}

@article{jones_characterising_2020,
  title        = {Characterising the Digital Twin: A systematic literature review},
  volume       = {29},
  issn         = {17555817},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S1755581720300110},
  doi          = {10.1016/j.cirpj.2020.02.002},
  shorttitle   = {Characterising the Digital Twin},
  pages        = {36--52},
  journaltitle = {{CIRP} Journal of Manufacturing Science and Technology},
  shortjournal = {{CIRP} Journal of Manufacturing Science and Technology},
  author       = {Jones, David and Snider, Chris and Nassehi, Aydin and Yon, Jason and Hicks, Ben},
  urldate      = {2025-07-31},
  date         = {2020-05},
  langid       = {english}
}

@article{mikolajewska_generative_2025,
  title        = {Generative {AI} in {AI}-Based Digital Twins for Fault Diagnosis for Predictive Maintenance in Industry 4.0/5.0},
  volume       = {15},
  rights       = {https://creativecommons.org/licenses/by/4.0/},
  issn         = {2076-3417},
  url          = {https://www.mdpi.com/2076-3417/15/6/3166},
  doi          = {10.3390/app15063166},
  abstract     = {Generative {AI} ({GenAI}) is revolutionizing digital twins ({DTs}) for fault diagnosis and predictive maintenance in Industry 4.0 and 5.0 by enabling real-time simulation, data augmentation, and improved anomaly detection. {DTs}, virtual replicas of physical systems, already use generative models to simulate various failure scenarios and rare events, improving system resilience and failure prediction accuracy. They create synthetic datasets that improve training quality while addressing data scarcity and data imbalance. The aim of this paper was to present the current state of the art and perspectives for using {AI}-based generative {DTs} for fault diagnosis for predictive maintenance in Industry 4.0/5.0. With {GenAI}, {DTs} enable proactive maintenance and minimize downtime, and their latest implementations combine multimodal sensor data to generate more realistic and actionable insights into system performance. This provides realistic operational profiles, identifying potential failure scenarios that traditional methods may miss. New perspectives in this area include the incorporation of Explainable {AI} ({XAI}) to increase transparency in decision-making and improve reliability in key industries such as manufacturing, energy, and healthcare. As Industry 5.0 emphasizes a human-centric approach, {AI}-based generative {DT} can seamlessly integrate with human operators to support collaboration and decision-making. The implementation of edge computing increases the scalability and real-time capabilities of {DTs} in smart factories and industrial Internet of Things ({IoT}) systems. Future advances may include federated learning to ensure data privacy while enabling data exchange between enterprises for fault diagnostics, and the evolution of {GenAI} alongside industrial systems, ensuring their long-term validity. However, challenges remain in managing computational complexity, ensuring data security, and addressing ethical issues during implementation.},
  pages        = {3166},
  number       = {6},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  author       = {Mikołajewska, Emilia and Mikołajewski, Dariusz and Mikołajczyk, Tadeusz and Paczkowski, Tomasz},
  urldate      = {2025-07-31},
  date         = {2025-03-14},
  langid       = {english}
}

@article{seid_ahmed_advances_2025,
  title        = {Advances in fault detection techniques for automated manufacturing systems in industry 4.0},
  volume       = {11},
  issn         = {2297-3079},
  url          = {https://www.frontiersin.org/articles/10.3389/fmech.2025.1564846/full},
  doi          = {10.3389/fmech.2025.1564846},
  abstract     = {Fault detection and diagnosis are essential for maintaining the continuous operation of manufacturing systems. To achieve this, an innovative tool is required to immediately identify any faults in the production process and recommend the appropriate mechanisms to be adopted proactively to prevent future mishaps or accidents. This capability is critical for many industries to improve the efficiency and effectiveness of their production processes. Several methods can be used to detect trends or patterns in any given process and determine if the process variable is within normal limits. However, these techniques may only detect evident process characteristics or defects while leaving behind latent ones. This paper aims to review recent achievements and classics in fault diagnosis and detection, and suggest steps that can be taken to plan and implement this process. It will also explore emerging research streams, critical issues in the field, and strategies that can be applied to overcome these barriers. The paper outlines how the performance of fault detection and diagnostics can be improved in production processes and how a safer and fully efficient production environment can be promoted.},
  pages        = {1564846},
  journaltitle = {Frontiers in Mechanical Engineering},
  shortjournal = {Front. Mech. Eng.},
  author       = {Seid Ahmed, Yassmin and Abubakar, Abba A. and Arif, Abul Fazal M. and Al-Badour, Fadi A.},
  urldate      = {2025-08-08},
  date         = {2025-04-17}
}

@article{wu_transformer-based_2023,
  title        = {A transformer-based approach for novel fault detection and fault classification/diagnosis in manufacturing: A rotary system application},
  volume       = {67},
  issn         = {02786125},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0278612523000419},
  doi          = {10.1016/j.jmsy.2023.02.018},
  shorttitle   = {A transformer-based approach for novel fault detection and fault classification/diagnosis in manufacturing},
  pages        = {439--452},
  journaltitle = {Journal of Manufacturing Systems},
  shortjournal = {Journal of Manufacturing Systems},
  author       = {Wu, Haiyue and Triebe, Matthew J. and Sutherland, John W.},
  urldate      = {2025-08-08},
  date         = {2023-04},
  langid       = {english}
}

@article{mercorelli_recent_2024,
  title        = {Recent Advances in Intelligent Algorithms for Fault Detection and Diagnosis},
  volume       = {24},
  rights       = {https://creativecommons.org/licenses/by/4.0/},
  issn         = {1424-8220},
  url          = {https://www.mdpi.com/1424-8220/24/8/2656},
  doi          = {10.3390/s24082656},
  abstract     = {Fault-finding diagnostics is a model-driven approach that identifies a system’s malfunctioning portion. It uses residual generators to identify faults, and various methods like isolation techniques and structural analysis are used. However, diagnostic equipment doesn’t measure the remaining signal-to-noise ratio. Residual selection identifies fault-detecting generators. Fault detective diagnostic ({FDD}) approaches have been investigated and implemented for various industrial processes. However, industrial operations make it difficult to implement {FDD} techniques. To bridge the gap between theoretical methodologies and implementations, hybrid approaches and intelligent procedures are needed. Future research should focus on improving fault prognosis, allowing for accurate prediction of process failures and avoiding safety hazards. Real-time and comprehensive {FDD} strategies should be implemented in the age of big data.},
  pages        = {2656},
  number       = {8},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  author       = {Mercorelli, Paolo},
  urldate      = {2025-08-08},
  date         = {2024-04-22},
  langid       = {english}
}

@article{wen_new_2018,
  title        = {A New Convolutional Neural Network-Based Data-Driven Fault Diagnosis Method},
  volume       = {65},
  rights       = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  issn         = {0278-0046, 1557-9948},
  url          = {http://ieeexplore.ieee.org/document/8114247/},
  doi          = {10.1109/TIE.2017.2774777},
  pages        = {5990--5998},
  number       = {7},
  journaltitle = {{IEEE} Transactions on Industrial Electronics},
  shortjournal = {{IEEE} Trans. Ind. Electron.},
  author       = {Wen, Long and Li, Xinyu and Gao, Liang and Zhang, Yuyan},
  urldate      = {2025-08-08},
  date         = {2018-07}
}

@incollection{vogel-heuser_remote_2024,
  location   = {Berlin, Heidelberg},
  title      = {Remote Operations: Fernüberwachung von Produktionsanlagen},
  isbn       = {978-3-662-58527-6 978-3-662-58528-3},
  url        = {https://link.springer.com/10.1007/978-3-662-58528-3_173},
  shorttitle = {Remote Operations},
  pages      = {591--598},
  booktitle  = {Handbuch Industrie 4.0},
  publisher  = {Springer Berlin Heidelberg},
  author     = {Trunzer, Emanuel and Pirehgalin, Mina Fahimi and Vogel-Heuser, Birgit and Odenweller, Matthias},
  editor     = {Vogel-Heuser, Birgit and Ten Hompel, Michael and Bauernhansl, Thomas},
  urldate    = {2025-08-12},
  date       = {2024},
  langid     = {german},
  doi        = {10.1007/978-3-662-58528-3_173}
}

@book{holtbrugge_remote_2007,
  location   = {Wiesbaden},
  edition    = {1. Aufl},
  title      = {Remote Services: neue Formen der Internationalisierung von Dienstleistungen},
  isbn       = {978-3-8350-9515-1},
  shorttitle = {Remote Services},
  publisher  = {Deutscher Universitäts-Verlag},
  author     = {Holtbrügge, Dirk and Holzmüller, Hartmut H. and Von Wangenheim, Florian},
  date       = {2007},
  note       = {{OCLC}: 401509773}
}

@article{ali_adversarial_2024,
  title        = {Adversarial Robustness of Vision Transformers Versus Convolutional Neural Networks},
  volume       = {12},
  rights       = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  issn         = {2169-3536},
  url          = {https://ieeexplore.ieee.org/document/10614176/},
  doi          = {10.1109/ACCESS.2024.3435347},
  pages        = {105281--105293},
  journaltitle = {{IEEE} Access},
  shortjournal = {{IEEE} Access},
  author       = {Ali, Kazim and Bhatti, Muhammad Shahid and Saeed, Atif and Athar, Atifa and Al Ghamdi, Mohammed A. and Almotiri, Sultan H. and Akram, Samina},
  urldate      = {2025-08-13},
  date         = {2024}
}

@article{arslanoglu_vision_2025,
  title        = {Vision Transformers Versus Convolutional Neural Networks: Comparing Robustness by Exploiting Varying Local Features},
  volume       = {13},
  rights       = {https://creativecommons.org/licenses/by/4.0/legalcode},
  issn         = {2169-3536},
  url          = {https://ieeexplore.ieee.org/document/10962160/},
  doi          = {10.1109/ACCESS.2025.3559794},
  shorttitle   = {Vision Transformers Versus Convolutional Neural Networks},
  pages        = {65232--65245},
  journaltitle = {{IEEE} Access},
  shortjournal = {{IEEE} Access},
  author       = {Arslanoglu, Muhammed Cihad and Albayrak, Abdulkadir and Acar, Huseyin},
  urldate      = {2025-08-13},
  date         = {2025}
}

@misc{dosovitskiy_image_2020,
  title      = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  rights     = {{arXiv}.org perpetual, non-exclusive license},
  url        = {https://arxiv.org/abs/2010.11929},
  doi        = {10.48550/ARXIV.2010.11929},
  shorttitle = {An Image is Worth 16x16 Words},
  abstract   = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  publisher  = {{arXiv}},
  author     = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  urldate    = {2025-08-15},
  date       = {2020},
  note       = {Version Number: 2},
  keywords   = {Artificial Intelligence (cs.{AI}), Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})}
}

@misc{bai_bridging_2023,
  title     = {Bridging the Domain Gap between Synthetic and Real-World Data for Autonomous Driving},
  rights    = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  url       = {https://arxiv.org/abs/2306.02631},
  doi       = {10.48550/ARXIV.2306.02631},
  abstract  = {Modern autonomous systems require extensive testing to ensure reliability and build trust in ground vehicles. However, testing these systems in the real-world is challenging due to the lack of large and diverse datasets, especially in edge cases. Therefore, simulations are necessary for their development and evaluation. However, existing open-source simulators often exhibit a significant gap between synthetic and real-world domains, leading to deteriorated mobility performance and reduced platform reliability when using simulation data. To address this issue, our Scoping Autonomous Vehicle Simulation ({SAVeS}) platform benchmarks the performance of simulated environments for autonomous ground vehicle testing between synthetic and real-world domains. Our platform aims to quantify the domain gap and enable researchers to develop and test autonomous systems in a controlled environment. Additionally, we propose using domain adaptation technologies to address the domain gap between synthetic and real-world data with our {SAVeS}\${\textasciicircum}+\$ extension. Our results demonstrate that {SAVeS}\${\textasciicircum}+\$ is effective in helping to close the gap between synthetic and real-world domains and yields comparable performance for models trained with processed synthetic datasets to those trained on real-world datasets of same scale. This paper highlights our efforts to quantify and address the domain gap between synthetic and real-world data for autonomy simulation. By enabling researchers to develop and test autonomous systems in a controlled environment, we hope to bring autonomy simulation one step closer to realization.},
  publisher = {{arXiv}},
  author    = {Bai, Xiangyu and Luo, Yedi and Jiang, Le and Gupta, Aniket and Kaveti, Pushyami and Singh, Hanumant and Ostadabbas, Sarah},
  urldate   = {2025-08-15},
  date      = {2023},
  note      = {Version Number: 1},
  keywords  = {{FOS}: Computer and information sciences, Robotics (cs.{RO})}
}

@article{hevner_design_2004,
  title        = {Design Science in Information Systems Research},
  volume       = {28},
  issn         = {02767783},
  url          = {https://www.jstor.org/stable/10.2307/25148625},
  doi          = {10.2307/25148625},
  pages        = {75},
  number       = {1},
  journaltitle = {{MIS} Quarterly},
  shortjournal = {{MIS} Quarterly},
  author       = {{Hevner} and {March} and {Park} and {Ram}},
  urldate      = {2025-08-15},
  date         = {2004}
}

@article{the_australian_national_university_positioning_2013,
  title        = {Positioning and Presenting Design Science Research for Maximum Impact},
  volume       = {37},
  issn         = {02767783, 21629730},
  url          = {https://misq.org/positioning-and-presenting-design-science-research-for-maximum-impact.html},
  doi          = {10.25300/MISQ/2013/37.2.01},
  pages        = {337--355},
  number       = {2},
  journaltitle = {{MIS} Quarterly},
  shortjournal = {{MISQ}},
  author       = {{The Australian National University} and Gregor, Shirley and Hevner, Alan R. and {University of South Florida}},
  urldate      = {2025-08-15},
  date         = {2013-02-02}
}

@article{peffers_design_2007,
  title        = {A Design Science Research Methodology for Information Systems Research},
  volume       = {24},
  issn         = {0742-1222, 1557-928X},
  url          = {https://www.tandfonline.com/doi/full/10.2753/MIS0742-1222240302},
  doi          = {10.2753/MIS0742-1222240302},
  pages        = {45--77},
  number       = {3},
  journaltitle = {Journal of Management Information Systems},
  shortjournal = {Journal of Management Information Systems},
  author       = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus A. and Chatterjee, Samir},
  urldate      = {2025-08-15},
  date         = {2007-12},
  langid       = {english}
}

@article{andrade_internal_2018,
  title        = {Internal, External, and Ecological Validity in Research Design, Conduct, and Evaluation},
  volume       = {40},
  issn         = {0253-7176, 0975-1564},
  url          = {https://journals.sagepub.com/doi/10.4103/IJPSYM.IJPSYM_334_18},
  doi          = {10.4103/IJPSYM.IJPSYM_334_18},
  pages        = {498--499},
  number       = {5},
  journaltitle = {Indian Journal of Psychological Medicine},
  shortjournal = {Indian Journal of Psychological Medicine},
  author       = {Andrade, Chittaranjan},
  urldate      = {2025-08-18},
  date         = {2018-09},
  langid       = {english}
}

@online{trumpf_se__co_kg_trumpf_2025,
  title   = {{TRUMPF SE \& Co. KG}},
  url     = {https://www.trumpf.com/de_DE/},
  author  = {{TRUMPF SE \& Co. KG}},
  urldate = {2025-08-20},
  date    = {2025-08-20}
}

@online{nvidia_isaac_2025,
  title   = {Isaac Sim Documentation},
  url     = {https://docs.isaacsim.omniverse.nvidia.com/latest/index.html},
  author  = {{NVIDIA}},
  urldate = {2025-08-20},
  date    = {2025-08-20}
}

@online{nvidia_omniverse_2025,
  title   = {Omniverse {USD} Composer},
  url     = {https://docs.omniverse.nvidia.com/composer/latest/index.html},
  author  = {{NVIDIA}},
  urldate = {2025-08-20},
  date    = {2025-08-20}
}

@online{nvidia_replicator_2025,
  title   = {Omniverse Replicator},
  url     = {https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html},
  author  = {{NVIDIA}},
  urldate = {2025-08-20},
  date    = {2025-08-20}
}

@software{yolo11_ultralytics,
  author  = {Glenn Jocher and Jing Qiu},
  title   = {Ultralytics YOLO11},
  version = {11.0.0},
  year    = {2024},
  url     = {https://github.com/ultralytics/ultralytics},
  orcid   = {0000-0001-5950-6979, 0000-0003-3783-7069},
  license = {AGPL-3.0}
}

@article{bilous_comparison_2024,
  title        = {Comparison of {CNN}-Based Architectures for Detection of Different Object Classes},
  volume       = {5},
  rights       = {https://creativecommons.org/licenses/by/4.0/},
  issn         = {2673-2688},
  url          = {https://www.mdpi.com/2673-2688/5/4/113},
  doi          = {10.3390/ai5040113},
  abstract     = {(1) Background: Detecting people and technical objects in various situations, such as natural disasters and warfare, is critical to search and rescue operations and the safety of civilians. A fast and accurate detection of people and equipment can significantly increase the effectiveness of search and rescue missions and provide timely assistance to people. Computer vision and deep learning technologies play a key role in detecting the required objects due to their ability to analyze big volumes of visual data in real-time. (2) Methods: The performance of the neural networks such as You Only Look Once ({YOLO}) v4-v8, Faster R-{CNN}, Single Shot {MultiBox} Detector ({SSD}), and {EfficientDet} has been analyzed using {COCO}2017, {SARD}, {SeaDronesSee}, and {VisDrone}2019 datasets. The main metrics for comparison were {mAP}, Precision, Recall, F1-Score, and the ability of the neural network to work in real-time. (3) Results: The most important metrics for evaluating the efficiency and performance of models for a given task are accuracy ({mAP}), F1-Score, and processing speed ({FPS}). These metrics allow us to evaluate both the accuracy of object recognition and the ability to use the models in real-world environments where high processing speed is important. (4) Conclusion: Although different neural networks perform better on certain types of metrics, {YOLO} outperforms them on all metrics, showing the best results of {mAP}-0.88, F1-0.88, and {FPS}-48, so the focus was on these models.},
  pages        = {2300--2320},
  number       = {4},
  journaltitle = {{AI}},
  shortjournal = {{AI}},
  author       = {Bilous, Nataliya and Malko, Vladyslav and Frohme, Marcus and Nechyporenko, Alina},
  urldate      = {2025-08-21},
  date         = {2024-11-11},
  langid       = {english}
}

@article{griem_synthetic_2025,
  title        = {Synthetic training data for {CT} image segmentation of microstructures},
  volume       = {296},
  issn         = {13596454},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S1359645425005075},
  doi          = {10.1016/j.actamat.2025.121220},
  pages        = {121220},
  journaltitle = {Acta Materialia},
  shortjournal = {Acta Materialia},
  author       = {Griem, Lars and Koeppe, Arnd and Greß, Alexander and Feser, Thomas and Nestler, Britta},
  urldate      = {2025-08-21},
  date         = {2025-09},
  langid       = {english}
}

@misc{khirodkar_domain_2018,
  title     = {Domain Randomization for Scene-Specific Car Detection and Pose Estimation},
  rights    = {{arXiv}.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1811.05939},
  doi       = {10.48550/ARXIV.1811.05939},
  abstract  = {We address the issue of domain gap when making use of synthetic data to train a scene-specific object detector and pose estimator. While previous works have shown that the constraints of learning a scene-specific model can be leveraged to create geometrically and photometrically consistent synthetic data, care must be taken to design synthetic content which is as close as possible to the real-world data distribution. In this work, we propose to solve domain gap through the use of appearance randomization to generate a wide range of synthetic objects to span the space of realistic images for training. An ablation study of our results is presented to delineate the individual contribution of different components in the randomization process. We evaluate our method on {VIRAT}, {UA}-{DETRAC}, {EPFL}-Car datasets, where we demonstrate that using scene specific domain randomized synthetic data is better than fine-tuning off-the-shelf models on limited real data.},
  publisher = {{arXiv}},
  author    = {Khirodkar, Rawal and Yoo, Donghyun and Kitani, Kris M.},
  urldate   = {2025-08-21},
  date      = {2018},
  note      = {Version Number: 1},
  keywords  = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences}
}

@misc{zhao_detrs_2023,
  title     = {{DETRs} Beat {YOLOs} on Real-time Object Detection},
  rights    = {{arXiv}.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2304.08069},
  doi       = {10.48550/ARXIV.2304.08069},
  abstract  = {The {YOLO} series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of {YOLOs} are negatively affected by the {NMS}. Recently, end-to-end Transformer-based detectors ({DETRs}) have provided an alternative to eliminating {NMS}. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding {NMS}. In this paper, we propose the Real-Time {DEtection} {TRansformer} ({RT}-{DETR}), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build {RT}-{DETR} in two steps, drawing on the advanced {DETR}: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, {RT}-{DETR} supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our {RT}-{DETR}-R50 / R101 achieves 53.1\% / 54.3\% {AP} on {COCO} and 108 / 74 {FPS} on T4 {GPU}, outperforming previously advanced {YOLOs} in both speed and accuracy. We also develop scaled {RT}-{DETRs} that outperform the lighter {YOLO} detectors (S and M models). Furthermore, {RT}-{DETR}-R50 outperforms {DINO}-R50 by 2.2\% {AP} in accuracy and about 21 times in {FPS}. After pre-training with Objects365, {RT}-{DETR}-R50 / R101 achieves 55.3\% / 56.2\% {AP}. The project page: https://zhao-yian.github.io/{RTDETR}.},
  publisher = {{arXiv}},
  author    = {Zhao, Yian and Lv, Wenyu and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Dang, Qingqing and Liu, Yi and Chen, Jie},
  urldate   = {2025-08-21},
  date      = {2023},
  note      = {Version Number: 3},
  keywords  = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences}
}

@online{ultralytics_hyperparameter-optimierung_nodate,
  title    = {Hyperparameter-Optimierung},
  url      = {https://docs.ultralytics.com/de/guides/hyperparameter-tuning},
  abstract = {Meistern Sie das Hyperparameter-Tuning für Ultralytics {YOLO}, um die Modellleistung mit unserem umfassenden Leitfaden zu optimieren. Optimieren Sie noch heute Ihre Modelle für maschinelles Lernen!.},
  author   = {Ultralytics},
  urldate  = {2025-08-22},
  langid   = {german},
  file     = {Snapshot:C\:\\Users\\Felix\\Zotero\\storage\\WPPD8YD6\\hyperparameter-tuning.html:text/html}
}

@online{ultralytics_yolo-datenerweiterung_nodate,
  title    = {{YOLO}-Datenerweiterung},
  url      = {https://docs.ultralytics.com/de/guides/yolo-data-augmentation},
  abstract = {Erfahre mehr über die wesentlichen Datenaugmentierungstechniken in Ultralytics {YOLO}. Entdecke verschiedene Transformationen, ihre Auswirkungen und wie du sie effektiv implementieren kannst, um die Modellleistung zu verbessern.},
  author   = {Ultralytics},
  urldate  = {2025-08-22},
  langid   = {german},
  file     = {Snapshot:C\:\\Users\\Felix\\Zotero\\storage\\C6UADUXX\\yolo-data-augmentation.html:text/html}
}

@article{khan_survey_2023,
  title        = {A survey of the vision transformers and their {CNN}-transformer based variants},
  volume       = {56},
  issn         = {0269-2821, 1573-7462},
  url          = {https://link.springer.com/10.1007/s10462-023-10595-0},
  doi          = {10.1007/s10462-023-10595-0},
  pages        = {2917--2970},
  issue        = {S3},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  author       = {Khan, Asifullah and Rauf, Zunaira and Sohail, Anabia and Khan, Abdul Rehman and Asif, Hifsa and Asif, Aqsa and Farooq, Umair},
  urldate      = {2025-07-31},
  date         = {2023-12},
  langid       = {english},
  file         = {Eingereichte Version:C\:\\Users\\Felix\\Zotero\\storage\\9WBHHTRU\\Khan et al. - 2023 - A survey of the vision transformers and their CNN-transformer based variants.pdf:application/pdf}
}

@online{noauthor_isaac_nodate,
  title   = {Isaac Sim Requirements — Isaac Sim Documentation},
  url     = {https://docs.isaacsim.omniverse.nvidia.com/latest/installation/requirements.html},
  urldate = {2025-08-24},
  file    = {Isaac Sim Requirements — Isaac Sim Documentation:C\:\\Users\\Felix\\Zotero\\storage\\RFVRHYU3\\requirements.html:text/html}
}

@software{noauthor_genesis-embodied-aigenesis_2025,
  title     = {Genesis-Embodied-{AI}/Genesis},
  rights    = {Apache-2.0},
  url       = {https://github.com/Genesis-Embodied-AI/Genesis},
  abstract  = {A generative world for general-purpose robotics \& embodied {AI} learning.},
  publisher = {Genesis {AI}},
  urldate   = {2025-08-24},
  date      = {2025-08-24},
  note      = {original-date: 2023-10-31T03:33:11Z}


}@misc{hosang_learning_2017,
	title = {Learning non-maximum suppression},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1705.02950},
	doi = {10.48550/ARXIV.1705.02950},
	abstract = {Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression ({NMS}), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard {NMS} algorithm is still fully hand-crafted, suspiciously simple, and -- being based on greedy clustering with a fixed distance threshold -- forces a trade-off between recall and precision. We propose a new network architecture designed to perform {NMS}, using only boxes and their score. We report experiments for person detection on {PETS} and for general object categories on the {COCO} dataset. Our approach shows promise providing improved localization and occlusion handling.},
	publisher = {{arXiv}},
	author = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
	urldate = {2025-08-26},
	date = {2017},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{he_mask_2017,
	title = {Mask R-{CNN}},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1703.06870},
	doi = {10.48550/ARXIV.1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-{CNN}, extends Faster R-{CNN} by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-{CNN} is simple to train and adds only a small overhead to Faster R-{CNN}, running at 5 fps. Moreover, Mask R-{CNN} is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the {COCO} suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-{CNN} outperforms all existing, single-model entries on every task, including the {COCO} 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	publisher = {{arXiv}},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	urldate = {2025-08-26},
	date = {2017},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{bodla_soft-nms_2017,
	title = {Soft-{NMS} -- Improving Object Detection With One Line of Code},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1704.04503},
	doi = {10.48550/ARXIV.1704.04503},
	abstract = {Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. To this end, we propose Soft-{NMS}, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-{NMS} obtains consistent improvements for the coco-style {mAP} metric on standard datasets like {PASCAL} {VOC} 2007 (1.7\% for both R-{FCN} and Faster-{RCNN}) and {MS}-{COCO} (1.3\% for R-{FCN} and 1.1\% for Faster-{RCNN}) by just changing the {NMS} algorithm without any additional hyper-parameters. Using Deformable-{RFCN}, Soft-{NMS} improves state-of-the-art in object detection from 39.8\% to 40.9\% with a single model. Further, the computational complexity of Soft-{NMS} is the same as traditional {NMS} and hence it can be efficiently implemented. Since Soft-{NMS} does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-{NMS} is publicly available on {GitHub} (http://bit.ly/2nJLNMu).},
	publisher = {{arXiv}},
	author = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S.},
	urldate = {2025-08-26},
	date = {2017},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@article{schmedemann_procedural_2022,
	title = {Procedural synthetic training data generation for {AI}-based defect detection in industrial surface inspection},
	volume = {107},
	issn = {22128271},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212827122003997},
	doi = {10.1016/j.procir.2022.05.115},
	pages = {1101--1106},
	journaltitle = {Procedia {CIRP}},
	shortjournal = {Procedia {CIRP}},
	author = {Schmedemann, Ole and Baaß, Melvin and Schoepflin, Daniel and Schüppstuhl, Thorsten},
	urldate = {2025-08-26},
	date = {2022},
	langid = {english},
}

@article{leite_fault_2024,
	title = {Fault Detection and Diagnosis in Industry 4.0: A Review on Challenges and Opportunities},
	volume = {25},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/25/1/60},
	doi = {10.3390/s25010060},
	shorttitle = {Fault Detection and Diagnosis in Industry 4.0},
	abstract = {Integrating Machine Learning ({ML}) in industrial settings has become a cornerstone of Industry 4.0, aiming to enhance production system reliability and efficiency through Real-Time Fault Detection and Diagnosis ({RT}-{FDD}). This paper conducts a comprehensive literature review of {ML}-based {RT}-{FDD}. Out of 805 documents, 29 studies were identified as noteworthy for presenting innovative methods that address the complexities and challenges associated with fault detection. While {ML}-based {RT}-{FDD} offers different benefits, including fault prediction accuracy, it faces challenges in data quality, model interpretability, and integration complexities. This review identifies a gap in industrial implementation outcomes that opens new research opportunities. Future Fault Detection and Diagnosis ({FDD}) research may prioritize standardized datasets to ensure reproducibility and facilitate comparative evaluations. Furthermore, there is a pressing need to refine techniques for handling unbalanced datasets and improving feature extraction for temporal series data. Implementing Explainable Artificial Intelligence ({AI}) ({XAI}) tailored to industrial fault detection is imperative for enhancing interpretability and trustworthiness. Subsequent studies must emphasize comprehensive comparative evaluations, reducing reliance on specialized expertise, documenting real-world outcomes, addressing data challenges, and bolstering real-time capabilities and integration. By addressing these avenues, the field can propel the advancement of {ML}-based {RT}-{FDD} methodologies, ensuring their effectiveness and relevance in industrial contexts.},
	pages = {60},
	number = {1},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Leite, Denis and Andrade, Emmanuel and Rativa, Diego and Maciel, Alexandre M. A.},
	urldate = {2025-08-26},
	date = {2024-12-25},
	langid = {english},
}
